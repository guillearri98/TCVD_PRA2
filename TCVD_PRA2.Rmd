---
title: 'Tipología y Ciclo de Vida de los Datos - PRA2'
author: "Guillermo Arrizabalaga y Javier Paredero"
date: "05/2022"
---

******
# Presentación
******
En esta práctica se elabora un caso práctico orientado a aprender a identificar los datos relevantes para un proyecto analítico y usar las herramientas de integración, limpieza, validación y análisis de las mismas.  

## Objetivos

Los objetivos concretos de esta práctica son:

* Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares.
* Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico.
* Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos.
* Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico.
* Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación.
* Desarrollar las habilidades de aprendizaje que les permitan continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo.
* Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos.

## Competencias

En esta práctica se desarrollan las siguientes competencias del Master de Data Science:

* Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo.
* Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración, transformación, limpieza y validación) para su posterior análisis.

******
# Resolución
******

## Descripción del dataset

Para la realización de esta práctica hemos escogido el dataset propuesto en el enunciado de la misma llamado **Titanic: Machine Learning from Disaster**. La elección de este dataset se debe a que el dataset creado en la PRA1 tenía muy pocos registros y variables para realizar un buen estudio.

Este dataset llamado train.csv, proporciona una serie de atributos acerca de los pasajeros del Titanic. Estos atributos se utilizan para predecir realizar predicciones sobre otros, utilizando modelos de aprendizaje supervisado. 

A continuación se explican los atributos que componen el dataset:

* PassengerId: Número que identifica al pasajero.
* Survived: Identifica si el pasajero sobrevivió (valor 1) o no (valor 0).
* Pclass: Idenfifica la clase del ticket como primera clase (valor 1), segunda clase (2) y tercera clase (3).
* Name: Nombre del pasajero.
* Sex: Sexo del pasajero.
* Age: Edad del pasajero.
* SibSp: Número de hermanos/cónyuges a bordo.
* Parch: Número de padres/hijos a bordo.
* Ticket: Número del ticket.
* Fare: Tarífa pagada.
* Cabin: Número de cabina.
* Embarked: Puerto donde embarcó.

Con este dataset se pretende estudiar la relación entre los atributos que influyen en mayor y menor medida a la posibilidad de supervivencia del pasajero. De esta manera podremos sacar conclusiones acerca de la diferencia entre géneros, clase social o edad a la hora de sobrevivir. 

## Integración y selección

Cargamos el fichero de datos:

```{r}
data <- read.csv('train.csv',stringsAsFactors = FALSE)
head(data)
dim(data)
```

El primer paso es decidir cuales de estos atributos vamos a utilizar en nuestro proyecto. En este caso para llevar a cabo el objetivo de este dataset vamos a prescindir de los atributos: PassengerId, Name, Ticket, Fare y Cabin:

```{r}
data <- data[,c(1,2,3,5,6,7,8,12)]
str(data)
dim(data)
```

Estos serán los datos con los que trabajaremos.

## Limpieza de los datos

El primer paso va a ser comprobar si nuestros datos contienen valores nulos o vacíos:

```{r}
summary(data)
```

La función summary ya nos revela que efectivamente existen datos nulos así que vamos a utilizar la función "is.na" para observar distintos casos:

```{r}
colSums(is.na(data))
colSums(data=="")
colSums(data=="?")
```

En este caso observamos que la variable "Age" contiene 177 valores nulos y la variable "Embarked" cotiene 2. Las dos filas pertenecientes a la variable "Embarked" vamos a eliminarlas mientras las filas pertenecientes a la variable "Age" vamos a completarlas mediante un método de imputación de vecinos. 

```{r}
if(!require(VIM)){
    install.packages('VIM', repos='http://cran.us.r-project.org')
    library(VIM)
}
data<- data[-data$PassengerId[data$Embarked==""],]
data$Age <- kNN(data)$Age
colSums(data=="")
colSums(is.na(data))
dim(data)
```

De esta manera ya no tenemos valores nulos en nuestros datos. El siguiente paso es ver si tenemos valores extremos. Para ello vamos a representar las variables numéricas:

```{r}
if(!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    library(ggplot2)
}

ggplot(data,aes(Survived))+geom_boxplot() 
ggplot(data,aes(Pclass))+geom_boxplot() 
ggplot(data,aes(Age))+geom_boxplot()
ggplot(data,aes(SibSp))+geom_boxplot()
ggplot(data,aes(Parch))+geom_boxplot()

```

En el caso las variables "SibSp", "Parch" y "Age" tienen valores extremos pero no los eliminamos puesto que son valores reales que serán útiles para nuestro modelo.

Una vez tenemos los datos limpios, los guardamos en un nuevo CSV:

```{r}
write.csv(data, "titanic_clean.csv")
```

## Análisis de los datos










